{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing module\n",
    "from pandas import *\n",
    " \n",
    "# reading CSV file\n",
    "data = read_csv(\"merged.csv\")\n",
    " \n",
    "# converting column data to list\n",
    "dir_x_list = data['dir_x'].tolist()\n",
    "dir_y_list = data['dir_y'].tolist()\n",
    "dir_z_list = data['dir_z'].tolist()\n",
    "vel1_x_list = data['vel1_x'].tolist()\n",
    "vel1_y_list = data['vel1_y'].tolist()\n",
    "vel1_z_list = data['vel1_z'].tolist()\n",
    "vel2_x_list = data['vel2_x'].tolist()\n",
    "vel2_y_list = data['vel2_y'].tolist()\n",
    "vel2_z_list = data['vel2_z'].tolist()\n",
    "frc_x_list = data['frc_x'].tolist()\n",
    "frc_y_list = data['frc_y'].tolist()\n",
    "frc_z_list = data['frc_z'].tolist()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################## Part 1: load data and create batch ##################\n",
    "# Generate data and shuffle\n",
    "N_total = 40000\n",
    "N_train = 30000\n",
    "\n",
    "# convert data list into tensor\n",
    "dir_x_ts = torch.FloatTensor(dir_x_list)\n",
    "dir_y_ts = torch.FloatTensor(dir_y_list)\n",
    "dir_z_ts = torch.FloatTensor(dir_z_list)\n",
    "\n",
    "vel1_x_ts = torch.FloatTensor(vel1_x_list)\n",
    "vel1_y_ts = torch.FloatTensor(vel1_y_list)\n",
    "vel1_z_ts = torch.FloatTensor(vel1_z_list)\n",
    "\n",
    "vel2_x_ts = torch.FloatTensor(vel2_x_list)\n",
    "vel2_y_ts = torch.FloatTensor(vel2_y_list)\n",
    "vel2_z_ts = torch.FloatTensor(vel2_z_list)\n",
    "\n",
    "frc_x_ts = torch.FloatTensor(frc_x_list)\n",
    "frc_y_ts = torch.FloatTensor(frc_y_list)\n",
    "frc_z_ts = torch.FloatTensor(frc_z_list)\n",
    "\n",
    "dir_x_ts_us = torch.unsqueeze(dir_x_ts,dim=1)\n",
    "dir_y_ts_us = torch.unsqueeze(dir_y_ts,dim=1)\n",
    "dir_z_ts_us = torch.unsqueeze(dir_z_ts,dim=1)\n",
    "\n",
    "vel1_x_ts_us = torch.unsqueeze(vel1_x_ts,dim=1)\n",
    "vel1_y_ts_us = torch.unsqueeze(vel1_y_ts,dim=1)\n",
    "vel1_z_ts_us = torch.unsqueeze(vel1_z_ts,dim=1)\n",
    "\n",
    "vel2_x_ts_us = torch.unsqueeze(vel2_x_ts,dim=1)\n",
    "vel2_y_ts_us = torch.unsqueeze(vel2_y_ts,dim=1)\n",
    "vel2_z_ts_us = torch.unsqueeze(vel2_z_ts,dim=1)\n",
    "\n",
    "frc_x_ts_us = torch.unsqueeze(frc_x_ts,dim=1)\n",
    "frc_y_ts_us = torch.unsqueeze(frc_y_ts,dim=1)\n",
    "frc_z_ts_us = torch.unsqueeze(frc_z_ts,dim=1)\n",
    "\n",
    "\n",
    "#x = torch.unsqueeze(torch.linspace(0, 1, N_total), dim=1)\n",
    "#r = torch.randperm(N_total)\n",
    "#x = x[r, :]\n",
    "#y = 0.2 + 0.4 * torch.pow(x, 2) + 0.3 * x * torch.sin(15 * x) + 0.05 * torch.cos(50 * x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.y = y\n",
    "        self.x = x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y1 = self.y[idx]\n",
    "        x1 = self.x[idx]\n",
    "        return (x1, y1)\n",
    "\n",
    "trainset = CustomDataset(x[0:N_train, :], y[0:N_train, :])\n",
    "testset = CustomDataset(x[N_train:N_total, :], y[N_train:N_total, :])\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=50)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1024, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=1024, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=1024, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ################## Part 2: Define Model and initialize ##################\n",
    "# This part need to be changed to define a new model\n",
    "model = nn.Sequential(nn.Linear(1, 1024, bias=True),\n",
    "                      nn.ReLU(),\n",
    "                      # nn.Linear(128, 64, bias=True),\n",
    "                      # nn.ReLU(),\n",
    "                      nn.Linear(1024, 1, bias=True)\n",
    "                      )\n",
    "print(model)\n",
    "\n",
    "\n",
    "# ############## This part can be changed to different initialization\n",
    "# Initialize as 0\n",
    "\n",
    "# Initialize as uniform [-1, 1]\n",
    "# for p in model.parameters():\n",
    "#     p.data.uniform_(-1, 1)\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.weight.data.uniform_(-1, 1)\n",
    "        m.bias.data.uniform_(-1, 1)\n",
    "\n",
    "# Initialize as normal\n",
    "# m.bias.data.normal_(0, 1)\n",
    "# m.weight.data.normal_(0, 0.03)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize as normal\n",
    "#\n",
    "# ########################################################################\n",
    "\n",
    "# ################## Part 3: Define Loss and optimizer ##################\n",
    "\n",
    "# ######## This can be changed to different loss function, e.g., L2loss\n",
    "# ######## and different optimization parameterï¼Œe.g. regularization, learning rata.\n",
    "criterion = torch.nn.MSELoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, train loss: 6.2081866, test loss: 6.2292633\n",
      "Epoch: 001, train loss: 3.4381480, test loss: 3.4500246\n",
      "Epoch: 002, train loss: 1.6520939, test loss: 1.6576262\n",
      "Epoch: 003, train loss: 0.6641168, test loss: 0.6657228\n",
      "Epoch: 004, train loss: 0.2138209, test loss: 0.2133635\n",
      "Epoch: 005, train loss: 0.0555182, test loss: 0.0541032\n",
      "Epoch: 006, train loss: 0.0184993, test loss: 0.0166960\n",
      "Epoch: 007, train loss: 0.0156620, test loss: 0.0137326\n",
      "Epoch: 008, train loss: 0.0166638, test loss: 0.0147169\n",
      "Epoch: 009, train loss: 0.0163136, test loss: 0.0143917\n",
      "Epoch: 010, train loss: 0.0155754, test loss: 0.0136917\n",
      "Epoch: 011, train loss: 0.0151522, test loss: 0.0133049\n",
      "Epoch: 012, train loss: 0.0149913, test loss: 0.0131731\n",
      "Epoch: 013, train loss: 0.0149063, test loss: 0.0131106\n",
      "Epoch: 014, train loss: 0.0148218, test loss: 0.0130439\n",
      "Epoch: 015, train loss: 0.0147329, test loss: 0.0129712\n",
      "Epoch: 016, train loss: 0.0146456, test loss: 0.0128998\n",
      "Epoch: 017, train loss: 0.0145598, test loss: 0.0128308\n",
      "Epoch: 018, train loss: 0.0144742, test loss: 0.0127625\n",
      "Epoch: 019, train loss: 0.0143885, test loss: 0.0126943\n",
      "Epoch: 020, train loss: 0.0143024, test loss: 0.0126262\n",
      "Epoch: 021, train loss: 0.0142160, test loss: 0.0125573\n",
      "Epoch: 022, train loss: 0.0141291, test loss: 0.0124873\n",
      "Epoch: 023, train loss: 0.0140417, test loss: 0.0124166\n",
      "Epoch: 024, train loss: 0.0139536, test loss: 0.0123454\n",
      "Epoch: 025, train loss: 0.0138650, test loss: 0.0122737\n",
      "Epoch: 026, train loss: 0.0137755, test loss: 0.0122011\n",
      "Epoch: 027, train loss: 0.0136853, test loss: 0.0121278\n",
      "Epoch: 028, train loss: 0.0135943, test loss: 0.0120539\n",
      "Epoch: 029, train loss: 0.0135024, test loss: 0.0119793\n",
      "Epoch: 030, train loss: 0.0134097, test loss: 0.0119037\n",
      "Epoch: 031, train loss: 0.0133163, test loss: 0.0118275\n",
      "Epoch: 032, train loss: 0.0132223, test loss: 0.0117504\n",
      "Epoch: 033, train loss: 0.0131274, test loss: 0.0116724\n",
      "Epoch: 034, train loss: 0.0130318, test loss: 0.0115938\n",
      "Epoch: 035, train loss: 0.0129355, test loss: 0.0115145\n",
      "Epoch: 036, train loss: 0.0128383, test loss: 0.0114348\n",
      "Epoch: 037, train loss: 0.0127407, test loss: 0.0113545\n",
      "Epoch: 038, train loss: 0.0126423, test loss: 0.0112733\n",
      "Epoch: 039, train loss: 0.0125431, test loss: 0.0111916\n",
      "Epoch: 040, train loss: 0.0124434, test loss: 0.0111095\n",
      "Epoch: 041, train loss: 0.0123431, test loss: 0.0110268\n",
      "Epoch: 042, train loss: 0.0122423, test loss: 0.0109434\n",
      "Epoch: 043, train loss: 0.0121409, test loss: 0.0108597\n",
      "Epoch: 044, train loss: 0.0120390, test loss: 0.0107756\n",
      "Epoch: 045, train loss: 0.0119366, test loss: 0.0106912\n",
      "Epoch: 046, train loss: 0.0118338, test loss: 0.0106065\n",
      "Epoch: 047, train loss: 0.0117305, test loss: 0.0105216\n",
      "Epoch: 048, train loss: 0.0116268, test loss: 0.0104363\n",
      "Epoch: 049, train loss: 0.0115226, test loss: 0.0103510\n",
      "Epoch: 050, train loss: 0.0114182, test loss: 0.0102654\n",
      "Epoch: 051, train loss: 0.0113135, test loss: 0.0101797\n",
      "Epoch: 052, train loss: 0.0112085, test loss: 0.0100936\n",
      "Epoch: 053, train loss: 0.0111034, test loss: 0.0100074\n",
      "Epoch: 054, train loss: 0.0109983, test loss: 0.0099207\n",
      "Epoch: 055, train loss: 0.0108933, test loss: 0.0098334\n",
      "Epoch: 056, train loss: 0.0107880, test loss: 0.0097460\n",
      "Epoch: 057, train loss: 0.0106828, test loss: 0.0096587\n",
      "Epoch: 058, train loss: 0.0105774, test loss: 0.0095714\n",
      "Epoch: 059, train loss: 0.0104720, test loss: 0.0094843\n",
      "Epoch: 060, train loss: 0.0103667, test loss: 0.0093972\n",
      "Epoch: 061, train loss: 0.0102614, test loss: 0.0093093\n",
      "Epoch: 062, train loss: 0.0101562, test loss: 0.0092217\n",
      "Epoch: 063, train loss: 0.0100510, test loss: 0.0091345\n",
      "Epoch: 064, train loss: 0.0099459, test loss: 0.0090476\n",
      "Epoch: 065, train loss: 0.0098408, test loss: 0.0089607\n",
      "Epoch: 066, train loss: 0.0097359, test loss: 0.0088737\n",
      "Epoch: 067, train loss: 0.0096312, test loss: 0.0087864\n",
      "Epoch: 068, train loss: 0.0095268, test loss: 0.0086983\n",
      "Epoch: 069, train loss: 0.0094229, test loss: 0.0086105\n",
      "Epoch: 070, train loss: 0.0093193, test loss: 0.0085233\n",
      "Epoch: 071, train loss: 0.0092161, test loss: 0.0084365\n",
      "Epoch: 072, train loss: 0.0091133, test loss: 0.0083492\n",
      "Epoch: 073, train loss: 0.0090109, test loss: 0.0082623\n",
      "Epoch: 074, train loss: 0.0089090, test loss: 0.0081759\n",
      "Epoch: 075, train loss: 0.0088075, test loss: 0.0080900\n",
      "Epoch: 076, train loss: 0.0087065, test loss: 0.0080039\n",
      "Epoch: 077, train loss: 0.0086060, test loss: 0.0079181\n",
      "Epoch: 078, train loss: 0.0085063, test loss: 0.0078326\n",
      "Epoch: 079, train loss: 0.0084074, test loss: 0.0077476\n",
      "Epoch: 080, train loss: 0.0083090, test loss: 0.0076629\n",
      "Epoch: 081, train loss: 0.0082112, test loss: 0.0075787\n",
      "Epoch: 082, train loss: 0.0081138, test loss: 0.0074948\n",
      "Epoch: 083, train loss: 0.0080173, test loss: 0.0074115\n",
      "Epoch: 084, train loss: 0.0079216, test loss: 0.0073287\n",
      "Epoch: 085, train loss: 0.0078266, test loss: 0.0072464\n",
      "Epoch: 086, train loss: 0.0077325, test loss: 0.0071644\n",
      "Epoch: 087, train loss: 0.0076392, test loss: 0.0070827\n",
      "Epoch: 088, train loss: 0.0075468, test loss: 0.0070018\n",
      "Epoch: 089, train loss: 0.0074551, test loss: 0.0069218\n",
      "Epoch: 090, train loss: 0.0073641, test loss: 0.0068426\n",
      "Epoch: 091, train loss: 0.0072740, test loss: 0.0067640\n",
      "Epoch: 092, train loss: 0.0071847, test loss: 0.0066862\n",
      "Epoch: 093, train loss: 0.0070962, test loss: 0.0066089\n",
      "Epoch: 094, train loss: 0.0070084, test loss: 0.0065325\n",
      "Epoch: 095, train loss: 0.0069216, test loss: 0.0064569\n",
      "Epoch: 096, train loss: 0.0068356, test loss: 0.0063821\n",
      "Epoch: 097, train loss: 0.0067504, test loss: 0.0063083\n",
      "Epoch: 098, train loss: 0.0066660, test loss: 0.0062350\n",
      "Epoch: 099, train loss: 0.0065823, test loss: 0.0061624\n",
      "Epoch: 100, train loss: 0.0064994, test loss: 0.0060910\n",
      "Epoch: 101, train loss: 0.0064174, test loss: 0.0060202\n",
      "Epoch: 102, train loss: 0.0063361, test loss: 0.0059500\n",
      "Epoch: 103, train loss: 0.0062557, test loss: 0.0058802\n",
      "Epoch: 104, train loss: 0.0061763, test loss: 0.0058108\n",
      "Epoch: 105, train loss: 0.0060977, test loss: 0.0057418\n",
      "Epoch: 106, train loss: 0.0060201, test loss: 0.0056733\n",
      "Epoch: 107, train loss: 0.0059431, test loss: 0.0056057\n",
      "Epoch: 108, train loss: 0.0058667, test loss: 0.0055390\n",
      "Epoch: 109, train loss: 0.0057910, test loss: 0.0054731\n",
      "Epoch: 110, train loss: 0.0057162, test loss: 0.0054076\n",
      "Epoch: 111, train loss: 0.0056420, test loss: 0.0053428\n",
      "Epoch: 112, train loss: 0.0055686, test loss: 0.0052789\n",
      "Epoch: 113, train loss: 0.0054958, test loss: 0.0052151\n",
      "Epoch: 114, train loss: 0.0054239, test loss: 0.0051518\n",
      "Epoch: 115, train loss: 0.0053527, test loss: 0.0050892\n",
      "Epoch: 116, train loss: 0.0052821, test loss: 0.0050273\n",
      "Epoch: 117, train loss: 0.0052122, test loss: 0.0049657\n",
      "Epoch: 118, train loss: 0.0051428, test loss: 0.0049046\n",
      "Epoch: 119, train loss: 0.0050744, test loss: 0.0048440\n",
      "Epoch: 120, train loss: 0.0050069, test loss: 0.0047838\n",
      "Epoch: 121, train loss: 0.0049400, test loss: 0.0047242\n",
      "Epoch: 122, train loss: 0.0048738, test loss: 0.0046650\n",
      "Epoch: 123, train loss: 0.0048083, test loss: 0.0046067\n",
      "Epoch: 124, train loss: 0.0047435, test loss: 0.0045487\n",
      "Epoch: 125, train loss: 0.0046794, test loss: 0.0044913\n",
      "Epoch: 126, train loss: 0.0046160, test loss: 0.0044345\n",
      "Epoch: 127, train loss: 0.0045533, test loss: 0.0043781\n",
      "Epoch: 128, train loss: 0.0044913, test loss: 0.0043222\n",
      "Epoch: 129, train loss: 0.0044299, test loss: 0.0042669\n",
      "Epoch: 130, train loss: 0.0043692, test loss: 0.0042123\n",
      "Epoch: 131, train loss: 0.0043091, test loss: 0.0041583\n",
      "Epoch: 132, train loss: 0.0042497, test loss: 0.0041048\n",
      "Epoch: 133, train loss: 0.0041909, test loss: 0.0040518\n",
      "Epoch: 134, train loss: 0.0041329, test loss: 0.0039995\n",
      "Epoch: 135, train loss: 0.0040755, test loss: 0.0039479\n",
      "Epoch: 136, train loss: 0.0040189, test loss: 0.0038967\n",
      "Epoch: 137, train loss: 0.0039630, test loss: 0.0038462\n",
      "Epoch: 138, train loss: 0.0039077, test loss: 0.0037963\n",
      "Epoch: 139, train loss: 0.0038530, test loss: 0.0037470\n",
      "Epoch: 140, train loss: 0.0037990, test loss: 0.0036984\n",
      "Epoch: 141, train loss: 0.0037456, test loss: 0.0036502\n",
      "Epoch: 142, train loss: 0.0036930, test loss: 0.0036028\n",
      "Epoch: 143, train loss: 0.0036411, test loss: 0.0035559\n",
      "Epoch: 144, train loss: 0.0035898, test loss: 0.0035096\n",
      "Epoch: 145, train loss: 0.0035392, test loss: 0.0034638\n",
      "Epoch: 146, train loss: 0.0034893, test loss: 0.0034187\n",
      "Epoch: 147, train loss: 0.0034400, test loss: 0.0033742\n",
      "Epoch: 148, train loss: 0.0033915, test loss: 0.0033302\n",
      "Epoch: 149, train loss: 0.0033436, test loss: 0.0032869\n",
      "Epoch: 150, train loss: 0.0032964, test loss: 0.0032442\n",
      "Epoch: 151, train loss: 0.0032498, test loss: 0.0032021\n",
      "Epoch: 152, train loss: 0.0032040, test loss: 0.0031607\n",
      "Epoch: 153, train loss: 0.0031588, test loss: 0.0031197\n",
      "Epoch: 154, train loss: 0.0031144, test loss: 0.0030795\n",
      "Epoch: 155, train loss: 0.0030707, test loss: 0.0030398\n",
      "Epoch: 156, train loss: 0.0030277, test loss: 0.0030008\n",
      "Epoch: 157, train loss: 0.0029853, test loss: 0.0029624\n",
      "Epoch: 158, train loss: 0.0029437, test loss: 0.0029247\n",
      "Epoch: 159, train loss: 0.0029027, test loss: 0.0028874\n",
      "Epoch: 160, train loss: 0.0028624, test loss: 0.0028507\n",
      "Epoch: 161, train loss: 0.0028228, test loss: 0.0028146\n",
      "Epoch: 162, train loss: 0.0027839, test loss: 0.0027790\n",
      "Epoch: 163, train loss: 0.0027456, test loss: 0.0027438\n",
      "Epoch: 164, train loss: 0.0027080, test loss: 0.0027093\n",
      "Epoch: 165, train loss: 0.0026710, test loss: 0.0026753\n",
      "Epoch: 166, train loss: 0.0026347, test loss: 0.0026419\n",
      "Epoch: 167, train loss: 0.0025989, test loss: 0.0026091\n",
      "Epoch: 168, train loss: 0.0025639, test loss: 0.0025769\n",
      "Epoch: 169, train loss: 0.0025294, test loss: 0.0025453\n",
      "Epoch: 170, train loss: 0.0024956, test loss: 0.0025142\n",
      "Epoch: 171, train loss: 0.0024623, test loss: 0.0024837\n",
      "Epoch: 172, train loss: 0.0024297, test loss: 0.0024536\n",
      "Epoch: 173, train loss: 0.0023977, test loss: 0.0024240\n",
      "Epoch: 174, train loss: 0.0023662, test loss: 0.0023947\n",
      "Epoch: 175, train loss: 0.0023353, test loss: 0.0023659\n",
      "Epoch: 176, train loss: 0.0023049, test loss: 0.0023375\n",
      "Epoch: 177, train loss: 0.0022751, test loss: 0.0023095\n",
      "Epoch: 178, train loss: 0.0022459, test loss: 0.0022821\n",
      "Epoch: 179, train loss: 0.0022172, test loss: 0.0022552\n",
      "Epoch: 180, train loss: 0.0021889, test loss: 0.0022287\n",
      "Epoch: 181, train loss: 0.0021612, test loss: 0.0022026\n",
      "Epoch: 182, train loss: 0.0021338, test loss: 0.0021770\n",
      "Epoch: 183, train loss: 0.0021070, test loss: 0.0021519\n",
      "Epoch: 184, train loss: 0.0020807, test loss: 0.0021273\n",
      "Epoch: 185, train loss: 0.0020549, test loss: 0.0021032\n",
      "Epoch: 186, train loss: 0.0020295, test loss: 0.0020796\n",
      "Epoch: 187, train loss: 0.0020044, test loss: 0.0020565\n",
      "Epoch: 188, train loss: 0.0019797, test loss: 0.0020338\n",
      "Epoch: 189, train loss: 0.0019554, test loss: 0.0020116\n",
      "Epoch: 190, train loss: 0.0019314, test loss: 0.0019897\n",
      "Epoch: 191, train loss: 0.0019078, test loss: 0.0019682\n",
      "Epoch: 192, train loss: 0.0018846, test loss: 0.0019472\n",
      "Epoch: 193, train loss: 0.0018618, test loss: 0.0019265\n",
      "Epoch: 194, train loss: 0.0018394, test loss: 0.0019062\n",
      "Epoch: 195, train loss: 0.0018173, test loss: 0.0018863\n",
      "Epoch: 196, train loss: 0.0017956, test loss: 0.0018667\n",
      "Epoch: 197, train loss: 0.0017743, test loss: 0.0018475\n",
      "Epoch: 198, train loss: 0.0017533, test loss: 0.0018286\n",
      "Epoch: 199, train loss: 0.0017325, test loss: 0.0018101\n",
      "Epoch: 200, train loss: 0.0017121, test loss: 0.0017918\n",
      "Epoch: 201, train loss: 0.0016921, test loss: 0.0017738\n",
      "Epoch: 202, train loss: 0.0016723, test loss: 0.0017561\n",
      "Epoch: 203, train loss: 0.0016528, test loss: 0.0017387\n",
      "Epoch: 204, train loss: 0.0016336, test loss: 0.0017216\n",
      "Epoch: 205, train loss: 0.0016148, test loss: 0.0017047\n",
      "Epoch: 206, train loss: 0.0015962, test loss: 0.0016882\n",
      "Epoch: 207, train loss: 0.0015778, test loss: 0.0016718\n",
      "Epoch: 208, train loss: 0.0015597, test loss: 0.0016556\n",
      "Epoch: 209, train loss: 0.0015419, test loss: 0.0016397\n",
      "Epoch: 210, train loss: 0.0015244, test loss: 0.0016239\n",
      "Epoch: 211, train loss: 0.0015072, test loss: 0.0016084\n",
      "Epoch: 212, train loss: 0.0014902, test loss: 0.0015930\n",
      "Epoch: 213, train loss: 0.0014735, test loss: 0.0015780\n",
      "Epoch: 214, train loss: 0.0014570, test loss: 0.0015631\n",
      "Epoch: 215, train loss: 0.0014407, test loss: 0.0015484\n",
      "Epoch: 216, train loss: 0.0014247, test loss: 0.0015340\n",
      "Epoch: 217, train loss: 0.0014089, test loss: 0.0015198\n",
      "Epoch: 218, train loss: 0.0013933, test loss: 0.0015058\n",
      "Epoch: 219, train loss: 0.0013780, test loss: 0.0014920\n",
      "Epoch: 220, train loss: 0.0013629, test loss: 0.0014784\n",
      "Epoch: 221, train loss: 0.0013481, test loss: 0.0014652\n",
      "Epoch: 222, train loss: 0.0013336, test loss: 0.0014523\n",
      "Epoch: 223, train loss: 0.0013194, test loss: 0.0014396\n",
      "Epoch: 224, train loss: 0.0013055, test loss: 0.0014273\n",
      "Epoch: 225, train loss: 0.0012919, test loss: 0.0014153\n",
      "Epoch: 226, train loss: 0.0012785, test loss: 0.0014036\n",
      "Epoch: 227, train loss: 0.0012653, test loss: 0.0013921\n",
      "Epoch: 228, train loss: 0.0012524, test loss: 0.0013808\n",
      "Epoch: 229, train loss: 0.0012399, test loss: 0.0013699\n",
      "Epoch: 230, train loss: 0.0012276, test loss: 0.0013591\n",
      "Epoch: 231, train loss: 0.0012155, test loss: 0.0013487\n",
      "Epoch: 232, train loss: 0.0012036, test loss: 0.0013385\n",
      "Epoch: 233, train loss: 0.0011920, test loss: 0.0013285\n",
      "Epoch: 234, train loss: 0.0011806, test loss: 0.0013187\n",
      "Epoch: 235, train loss: 0.0011694, test loss: 0.0013090\n",
      "Epoch: 236, train loss: 0.0011584, test loss: 0.0012994\n",
      "Epoch: 237, train loss: 0.0011476, test loss: 0.0012900\n",
      "Epoch: 238, train loss: 0.0011370, test loss: 0.0012808\n",
      "Epoch: 239, train loss: 0.0011266, test loss: 0.0012718\n",
      "Epoch: 240, train loss: 0.0011163, test loss: 0.0012628\n",
      "Epoch: 241, train loss: 0.0011062, test loss: 0.0012540\n",
      "Epoch: 242, train loss: 0.0010963, test loss: 0.0012455\n",
      "Epoch: 243, train loss: 0.0010866, test loss: 0.0012371\n",
      "Epoch: 244, train loss: 0.0010770, test loss: 0.0012287\n",
      "Epoch: 245, train loss: 0.0010677, test loss: 0.0012206\n",
      "Epoch: 246, train loss: 0.0010585, test loss: 0.0012126\n",
      "Epoch: 247, train loss: 0.0010494, test loss: 0.0012048\n",
      "Epoch: 248, train loss: 0.0010405, test loss: 0.0011970\n",
      "Epoch: 249, train loss: 0.0010318, test loss: 0.0011895\n",
      "Epoch: 250, train loss: 0.0010231, test loss: 0.0011820\n",
      "Epoch: 251, train loss: 0.0010147, test loss: 0.0011747\n",
      "Epoch: 252, train loss: 0.0010064, test loss: 0.0011676\n",
      "Epoch: 253, train loss: 0.0009982, test loss: 0.0011607\n",
      "Epoch: 254, train loss: 0.0009902, test loss: 0.0011538\n",
      "Epoch: 255, train loss: 0.0009823, test loss: 0.0011470\n",
      "Epoch: 256, train loss: 0.0009745, test loss: 0.0011402\n",
      "Epoch: 257, train loss: 0.0009669, test loss: 0.0011337\n",
      "Epoch: 258, train loss: 0.0009595, test loss: 0.0011273\n",
      "Epoch: 259, train loss: 0.0009521, test loss: 0.0011209\n",
      "Epoch: 260, train loss: 0.0009449, test loss: 0.0011146\n",
      "Epoch: 261, train loss: 0.0009378, test loss: 0.0011084\n",
      "Epoch: 262, train loss: 0.0009308, test loss: 0.0011022\n",
      "Epoch: 263, train loss: 0.0009240, test loss: 0.0010961\n",
      "Epoch: 264, train loss: 0.0009173, test loss: 0.0010902\n",
      "Epoch: 265, train loss: 0.0009107, test loss: 0.0010845\n",
      "Epoch: 266, train loss: 0.0009042, test loss: 0.0010788\n",
      "Epoch: 267, train loss: 0.0008978, test loss: 0.0010731\n",
      "Epoch: 268, train loss: 0.0008914, test loss: 0.0010675\n",
      "Epoch: 269, train loss: 0.0008852, test loss: 0.0010620\n",
      "Epoch: 270, train loss: 0.0008791, test loss: 0.0010565\n",
      "Epoch: 271, train loss: 0.0008730, test loss: 0.0010511\n",
      "Epoch: 272, train loss: 0.0008671, test loss: 0.0010458\n",
      "Epoch: 273, train loss: 0.0008613, test loss: 0.0010406\n",
      "Epoch: 274, train loss: 0.0008556, test loss: 0.0010356\n",
      "Epoch: 275, train loss: 0.0008499, test loss: 0.0010306\n",
      "Epoch: 276, train loss: 0.0008444, test loss: 0.0010256\n",
      "Epoch: 277, train loss: 0.0008389, test loss: 0.0010206\n",
      "Epoch: 278, train loss: 0.0008336, test loss: 0.0010157\n",
      "Epoch: 279, train loss: 0.0008282, test loss: 0.0010109\n",
      "Epoch: 280, train loss: 0.0008230, test loss: 0.0010061\n",
      "Epoch: 281, train loss: 0.0008179, test loss: 0.0010014\n",
      "Epoch: 282, train loss: 0.0008128, test loss: 0.0009968\n",
      "Epoch: 283, train loss: 0.0008078, test loss: 0.0009922\n",
      "Epoch: 284, train loss: 0.0008029, test loss: 0.0009876\n",
      "Epoch: 285, train loss: 0.0007981, test loss: 0.0009834\n",
      "Epoch: 286, train loss: 0.0007933, test loss: 0.0009790\n",
      "Epoch: 287, train loss: 0.0007885, test loss: 0.0009746\n",
      "Epoch: 288, train loss: 0.0007839, test loss: 0.0009703\n",
      "Epoch: 289, train loss: 0.0007793, test loss: 0.0009661\n",
      "Epoch: 290, train loss: 0.0007748, test loss: 0.0009619\n",
      "Epoch: 291, train loss: 0.0007703, test loss: 0.0009577\n",
      "Epoch: 292, train loss: 0.0007660, test loss: 0.0009536\n",
      "Epoch: 293, train loss: 0.0007616, test loss: 0.0009495\n",
      "Epoch: 294, train loss: 0.0007574, test loss: 0.0009455\n",
      "Epoch: 295, train loss: 0.0007532, test loss: 0.0009415\n",
      "Epoch: 296, train loss: 0.0007490, test loss: 0.0009376\n",
      "Epoch: 297, train loss: 0.0007449, test loss: 0.0009337\n",
      "Epoch: 298, train loss: 0.0007409, test loss: 0.0009298\n",
      "Epoch: 299, train loss: 0.0007368, test loss: 0.0009260\n",
      "Epoch: 300, train loss: 0.0007329, test loss: 0.0009222\n",
      "Epoch: 301, train loss: 0.0007290, test loss: 0.0009185\n",
      "Epoch: 302, train loss: 0.0007251, test loss: 0.0009148\n",
      "Epoch: 303, train loss: 0.0007213, test loss: 0.0009111\n",
      "Epoch: 304, train loss: 0.0007175, test loss: 0.0009075\n",
      "Epoch: 305, train loss: 0.0007138, test loss: 0.0009039\n",
      "Epoch: 306, train loss: 0.0007101, test loss: 0.0009005\n",
      "Epoch: 307, train loss: 0.0007065, test loss: 0.0008971\n",
      "Epoch: 308, train loss: 0.0007029, test loss: 0.0008936\n",
      "Epoch: 309, train loss: 0.0006993, test loss: 0.0008902\n",
      "Epoch: 310, train loss: 0.0006957, test loss: 0.0008867\n",
      "Epoch: 311, train loss: 0.0006922, test loss: 0.0008834\n",
      "Epoch: 312, train loss: 0.0006887, test loss: 0.0008800\n",
      "Epoch: 313, train loss: 0.0006853, test loss: 0.0008767\n",
      "Epoch: 314, train loss: 0.0006819, test loss: 0.0008734\n",
      "Epoch: 315, train loss: 0.0006785, test loss: 0.0008701\n",
      "Epoch: 316, train loss: 0.0006752, test loss: 0.0008667\n",
      "Epoch: 317, train loss: 0.0006719, test loss: 0.0008635\n",
      "Epoch: 318, train loss: 0.0006687, test loss: 0.0008603\n",
      "Epoch: 319, train loss: 0.0006654, test loss: 0.0008570\n",
      "Epoch: 320, train loss: 0.0006623, test loss: 0.0008539\n",
      "Epoch: 321, train loss: 0.0006591, test loss: 0.0008507\n",
      "Epoch: 322, train loss: 0.0006560, test loss: 0.0008475\n",
      "Epoch: 323, train loss: 0.0006529, test loss: 0.0008443\n",
      "Epoch: 324, train loss: 0.0006498, test loss: 0.0008412\n",
      "Epoch: 325, train loss: 0.0006468, test loss: 0.0008382\n",
      "Epoch: 326, train loss: 0.0006438, test loss: 0.0008352\n",
      "Epoch: 327, train loss: 0.0006408, test loss: 0.0008320\n",
      "Epoch: 328, train loss: 0.0006378, test loss: 0.0008291\n",
      "Epoch: 329, train loss: 0.0006349, test loss: 0.0008261\n",
      "Epoch: 330, train loss: 0.0006320, test loss: 0.0008231\n",
      "Epoch: 331, train loss: 0.0006291, test loss: 0.0008201\n",
      "Epoch: 332, train loss: 0.0006263, test loss: 0.0008172\n",
      "Epoch: 333, train loss: 0.0006235, test loss: 0.0008143\n",
      "Epoch: 334, train loss: 0.0006207, test loss: 0.0008113\n",
      "Epoch: 335, train loss: 0.0006179, test loss: 0.0008084\n",
      "Epoch: 336, train loss: 0.0006151, test loss: 0.0008056\n",
      "Epoch: 337, train loss: 0.0006124, test loss: 0.0008027\n",
      "Epoch: 338, train loss: 0.0006096, test loss: 0.0007999\n",
      "Epoch: 339, train loss: 0.0006069, test loss: 0.0007971\n",
      "Epoch: 340, train loss: 0.0006042, test loss: 0.0007944\n",
      "Epoch: 341, train loss: 0.0006016, test loss: 0.0007916\n",
      "Epoch: 342, train loss: 0.0005989, test loss: 0.0007889\n",
      "Epoch: 343, train loss: 0.0005963, test loss: 0.0007861\n",
      "Epoch: 344, train loss: 0.0005937, test loss: 0.0007834\n",
      "Epoch: 345, train loss: 0.0005911, test loss: 0.0007807\n",
      "Epoch: 346, train loss: 0.0005885, test loss: 0.0007779\n",
      "Epoch: 347, train loss: 0.0005859, test loss: 0.0007752\n",
      "Epoch: 348, train loss: 0.0005833, test loss: 0.0007725\n",
      "Epoch: 349, train loss: 0.0005808, test loss: 0.0007698\n",
      "Epoch: 350, train loss: 0.0005783, test loss: 0.0007671\n",
      "Epoch: 351, train loss: 0.0005757, test loss: 0.0007644\n",
      "Epoch: 352, train loss: 0.0005732, test loss: 0.0007617\n",
      "Epoch: 353, train loss: 0.0005707, test loss: 0.0007591\n",
      "Epoch: 354, train loss: 0.0005683, test loss: 0.0007564\n",
      "Epoch: 355, train loss: 0.0005658, test loss: 0.0007537\n",
      "Epoch: 356, train loss: 0.0005634, test loss: 0.0007511\n",
      "Epoch: 357, train loss: 0.0005609, test loss: 0.0007484\n",
      "Epoch: 358, train loss: 0.0005585, test loss: 0.0007458\n",
      "Epoch: 359, train loss: 0.0005561, test loss: 0.0007431\n",
      "Epoch: 360, train loss: 0.0005537, test loss: 0.0007405\n",
      "Epoch: 361, train loss: 0.0005514, test loss: 0.0007378\n",
      "Epoch: 362, train loss: 0.0005490, test loss: 0.0007352\n",
      "Epoch: 363, train loss: 0.0005467, test loss: 0.0007325\n",
      "Epoch: 364, train loss: 0.0005444, test loss: 0.0007298\n",
      "Epoch: 365, train loss: 0.0005421, test loss: 0.0007272\n",
      "Epoch: 366, train loss: 0.0005398, test loss: 0.0007245\n",
      "Epoch: 367, train loss: 0.0005375, test loss: 0.0007219\n",
      "Epoch: 368, train loss: 0.0005353, test loss: 0.0007193\n",
      "Epoch: 369, train loss: 0.0005330, test loss: 0.0007167\n",
      "Epoch: 370, train loss: 0.0005308, test loss: 0.0007141\n",
      "Epoch: 371, train loss: 0.0005285, test loss: 0.0007115\n",
      "Epoch: 372, train loss: 0.0005263, test loss: 0.0007089\n",
      "Epoch: 373, train loss: 0.0005241, test loss: 0.0007064\n",
      "Epoch: 374, train loss: 0.0005218, test loss: 0.0007038\n",
      "Epoch: 375, train loss: 0.0005196, test loss: 0.0007012\n",
      "Epoch: 376, train loss: 0.0005174, test loss: 0.0006986\n",
      "Epoch: 377, train loss: 0.0005152, test loss: 0.0006961\n",
      "Epoch: 378, train loss: 0.0005130, test loss: 0.0006936\n",
      "Epoch: 379, train loss: 0.0005108, test loss: 0.0006910\n",
      "Epoch: 380, train loss: 0.0005086, test loss: 0.0006885\n",
      "Epoch: 381, train loss: 0.0005064, test loss: 0.0006860\n",
      "Epoch: 382, train loss: 0.0005043, test loss: 0.0006834\n",
      "Epoch: 383, train loss: 0.0005021, test loss: 0.0006809\n",
      "Epoch: 384, train loss: 0.0005000, test loss: 0.0006783\n",
      "Epoch: 385, train loss: 0.0004978, test loss: 0.0006759\n",
      "Epoch: 386, train loss: 0.0004957, test loss: 0.0006734\n",
      "Epoch: 387, train loss: 0.0004936, test loss: 0.0006709\n",
      "Epoch: 388, train loss: 0.0004915, test loss: 0.0006684\n",
      "Epoch: 389, train loss: 0.0004894, test loss: 0.0006659\n",
      "Epoch: 390, train loss: 0.0004873, test loss: 0.0006634\n",
      "Epoch: 391, train loss: 0.0004852, test loss: 0.0006609\n",
      "Epoch: 392, train loss: 0.0004830, test loss: 0.0006584\n",
      "Epoch: 393, train loss: 0.0004810, test loss: 0.0006559\n",
      "Epoch: 394, train loss: 0.0004789, test loss: 0.0006534\n",
      "Epoch: 395, train loss: 0.0004768, test loss: 0.0006509\n",
      "Epoch: 396, train loss: 0.0004748, test loss: 0.0006484\n",
      "Epoch: 397, train loss: 0.0004727, test loss: 0.0006459\n",
      "Epoch: 398, train loss: 0.0004707, test loss: 0.0006435\n",
      "Epoch: 399, train loss: 0.0004686, test loss: 0.0006409\n",
      "Epoch: 400, train loss: 0.0004666, test loss: 0.0006385\n",
      "Epoch: 401, train loss: 0.0004646, test loss: 0.0006361\n",
      "Epoch: 402, train loss: 0.0004626, test loss: 0.0006336\n",
      "Epoch: 403, train loss: 0.0004605, test loss: 0.0006312\n",
      "Epoch: 404, train loss: 0.0004585, test loss: 0.0006287\n",
      "Epoch: 405, train loss: 0.0004565, test loss: 0.0006262\n",
      "Epoch: 406, train loss: 0.0004545, test loss: 0.0006239\n",
      "Epoch: 407, train loss: 0.0004525, test loss: 0.0006214\n",
      "Epoch: 408, train loss: 0.0004505, test loss: 0.0006190\n",
      "Epoch: 409, train loss: 0.0004486, test loss: 0.0006167\n",
      "Epoch: 410, train loss: 0.0004466, test loss: 0.0006142\n",
      "Epoch: 411, train loss: 0.0004446, test loss: 0.0006118\n",
      "Epoch: 412, train loss: 0.0004427, test loss: 0.0006095\n",
      "Epoch: 413, train loss: 0.0004407, test loss: 0.0006071\n",
      "Epoch: 414, train loss: 0.0004387, test loss: 0.0006046\n",
      "Epoch: 415, train loss: 0.0004368, test loss: 0.0006023\n",
      "Epoch: 416, train loss: 0.0004349, test loss: 0.0005999\n",
      "Epoch: 417, train loss: 0.0004329, test loss: 0.0005974\n",
      "Epoch: 418, train loss: 0.0004310, test loss: 0.0005950\n",
      "Epoch: 419, train loss: 0.0004291, test loss: 0.0005927\n",
      "Epoch: 420, train loss: 0.0004272, test loss: 0.0005903\n",
      "Epoch: 421, train loss: 0.0004253, test loss: 0.0005879\n",
      "Epoch: 422, train loss: 0.0004234, test loss: 0.0005855\n",
      "Epoch: 423, train loss: 0.0004214, test loss: 0.0005832\n",
      "Epoch: 424, train loss: 0.0004195, test loss: 0.0005808\n",
      "Epoch: 425, train loss: 0.0004177, test loss: 0.0005784\n",
      "Epoch: 426, train loss: 0.0004158, test loss: 0.0005760\n",
      "Epoch: 427, train loss: 0.0004139, test loss: 0.0005737\n",
      "Epoch: 428, train loss: 0.0004120, test loss: 0.0005713\n",
      "Epoch: 429, train loss: 0.0004101, test loss: 0.0005689\n",
      "Epoch: 430, train loss: 0.0004083, test loss: 0.0005666\n",
      "Epoch: 431, train loss: 0.0004064, test loss: 0.0005641\n",
      "Epoch: 432, train loss: 0.0004045, test loss: 0.0005618\n",
      "Epoch: 433, train loss: 0.0004027, test loss: 0.0005595\n",
      "Epoch: 434, train loss: 0.0004008, test loss: 0.0005571\n",
      "Epoch: 435, train loss: 0.0003990, test loss: 0.0005548\n",
      "Epoch: 436, train loss: 0.0003971, test loss: 0.0005524\n",
      "Epoch: 437, train loss: 0.0003953, test loss: 0.0005501\n",
      "Epoch: 438, train loss: 0.0003935, test loss: 0.0005477\n",
      "Epoch: 439, train loss: 0.0003916, test loss: 0.0005454\n",
      "Epoch: 440, train loss: 0.0003898, test loss: 0.0005431\n",
      "Epoch: 441, train loss: 0.0003880, test loss: 0.0005407\n",
      "Epoch: 442, train loss: 0.0003862, test loss: 0.0005383\n",
      "Epoch: 443, train loss: 0.0003844, test loss: 0.0005361\n",
      "Epoch: 444, train loss: 0.0003826, test loss: 0.0005337\n",
      "Epoch: 445, train loss: 0.0003808, test loss: 0.0005313\n",
      "Epoch: 446, train loss: 0.0003791, test loss: 0.0005290\n",
      "Epoch: 447, train loss: 0.0003773, test loss: 0.0005269\n",
      "Epoch: 448, train loss: 0.0003755, test loss: 0.0005245\n",
      "Epoch: 449, train loss: 0.0003738, test loss: 0.0005222\n",
      "Epoch: 450, train loss: 0.0003720, test loss: 0.0005200\n",
      "Epoch: 451, train loss: 0.0003702, test loss: 0.0005177\n",
      "Epoch: 452, train loss: 0.0003685, test loss: 0.0005155\n",
      "Epoch: 453, train loss: 0.0003667, test loss: 0.0005132\n",
      "Epoch: 454, train loss: 0.0003650, test loss: 0.0005110\n",
      "Epoch: 455, train loss: 0.0003632, test loss: 0.0005088\n",
      "Epoch: 456, train loss: 0.0003615, test loss: 0.0005065\n",
      "Epoch: 457, train loss: 0.0003598, test loss: 0.0005043\n",
      "Epoch: 458, train loss: 0.0003580, test loss: 0.0005021\n",
      "Epoch: 459, train loss: 0.0003563, test loss: 0.0004999\n",
      "Epoch: 460, train loss: 0.0003546, test loss: 0.0004976\n",
      "Epoch: 461, train loss: 0.0003529, test loss: 0.0004955\n",
      "Epoch: 462, train loss: 0.0003512, test loss: 0.0004933\n",
      "Epoch: 463, train loss: 0.0003495, test loss: 0.0004910\n",
      "Epoch: 464, train loss: 0.0003478, test loss: 0.0004888\n",
      "Epoch: 465, train loss: 0.0003461, test loss: 0.0004867\n",
      "Epoch: 466, train loss: 0.0003445, test loss: 0.0004845\n",
      "Epoch: 467, train loss: 0.0003428, test loss: 0.0004823\n",
      "Epoch: 468, train loss: 0.0003411, test loss: 0.0004802\n",
      "Epoch: 469, train loss: 0.0003395, test loss: 0.0004780\n",
      "Epoch: 470, train loss: 0.0003378, test loss: 0.0004758\n",
      "Epoch: 471, train loss: 0.0003362, test loss: 0.0004736\n",
      "Epoch: 472, train loss: 0.0003345, test loss: 0.0004715\n",
      "Epoch: 473, train loss: 0.0003329, test loss: 0.0004693\n",
      "Epoch: 474, train loss: 0.0003312, test loss: 0.0004672\n",
      "Epoch: 475, train loss: 0.0003296, test loss: 0.0004650\n",
      "Epoch: 476, train loss: 0.0003280, test loss: 0.0004629\n",
      "Epoch: 477, train loss: 0.0003264, test loss: 0.0004607\n",
      "Epoch: 478, train loss: 0.0003248, test loss: 0.0004585\n",
      "Epoch: 479, train loss: 0.0003232, test loss: 0.0004564\n",
      "Epoch: 480, train loss: 0.0003216, test loss: 0.0004542\n",
      "Epoch: 481, train loss: 0.0003200, test loss: 0.0004521\n",
      "Epoch: 482, train loss: 0.0003184, test loss: 0.0004500\n",
      "Epoch: 483, train loss: 0.0003168, test loss: 0.0004478\n",
      "Epoch: 484, train loss: 0.0003152, test loss: 0.0004457\n",
      "Epoch: 485, train loss: 0.0003137, test loss: 0.0004436\n",
      "Epoch: 486, train loss: 0.0003121, test loss: 0.0004414\n",
      "Epoch: 487, train loss: 0.0003106, test loss: 0.0004394\n",
      "Epoch: 488, train loss: 0.0003090, test loss: 0.0004372\n",
      "Epoch: 489, train loss: 0.0003075, test loss: 0.0004351\n",
      "Epoch: 490, train loss: 0.0003060, test loss: 0.0004330\n",
      "Epoch: 491, train loss: 0.0003045, test loss: 0.0004309\n",
      "Epoch: 492, train loss: 0.0003030, test loss: 0.0004288\n",
      "Epoch: 493, train loss: 0.0003015, test loss: 0.0004267\n",
      "Epoch: 494, train loss: 0.0003000, test loss: 0.0004246\n",
      "Epoch: 495, train loss: 0.0002985, test loss: 0.0004226\n",
      "Epoch: 496, train loss: 0.0002970, test loss: 0.0004206\n",
      "Epoch: 497, train loss: 0.0002955, test loss: 0.0004185\n",
      "Epoch: 498, train loss: 0.0002940, test loss: 0.0004164\n",
      "Epoch: 499, train loss: 0.0002925, test loss: 0.0004144\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ################## Part 4: Optimization ##################\n",
    "def train_NN():\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        out = model(images)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test_NN(loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    for images, labels in loader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        out = model(images)\n",
    "        loss += criterion(out, labels)\n",
    "    loss = loss / len(loader)\n",
    "    return loss\n",
    "\n",
    "\n",
    "N_epoch = 500\n",
    "train_loss = np.zeros((N_epoch, 1))\n",
    "test_loss = np.zeros((N_epoch, 1))\n",
    "for epoch in range(N_epoch):\n",
    "    loss1 = train_NN()\n",
    "    train_loss[epoch, 0] = test_NN(train_loader)\n",
    "    test_loss[epoch, 0] = test_NN(test_loader)\n",
    "    print(\n",
    "        f'Epoch: {epoch:03d}, train loss: {train_loss[epoch, 0]:.7f}, test loss: {test_loss[epoch, 0]:.7f}')\n",
    "\n",
    "x_test = torch.unsqueeze(torch.linspace(0, 1, 1999), dim=1)\n",
    "y_test = model(x_test)\n",
    "\n",
    "plt.plot(x[0:N_total], y[0:N_total], 'bo')\n",
    "plt.plot(x_test, y_test.detach().numpy(), 'r')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
